{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a5edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "from typing import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1a8349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (100, 4)\n",
      "Shape of y: (100,)\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "#shape\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#making the dataset into binary for binary classification\n",
    "binary_filter = y < 2\n",
    "X = X[binary_filter]\n",
    "y = y[binary_filter]\n",
    "\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "\n",
    "X = (X - mean) / std\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41711579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (80, 4)\n",
      "Shape of X_test: (20, 4)\n",
      "Shape of Y_train: (80,)\n",
      "Shape of Y_test: (20,)\n"
     ]
    }
   ],
   "source": [
    "#splitting into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=55)\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "print(\"Shape of Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9998390",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.T \n",
    "X_test = X_test.T\n",
    "\n",
    "Y_train = Y_train.reshape(1, -1)\n",
    "Y_test = Y_test.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25600f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c9a4079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0) #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018f2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim: int) -> Tuple[np.ndarray, float]:\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0.0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbfeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w: np.ndarray, b: float, X: np.ndarray, Y: np.ndarray) -> Tuple[Dict[str, np.ndarray], float]:\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    assert w.shape == (X.shape[0], 1)  #w -> (num_features, 1)\n",
    "    assert X.shape[0] == w.shape[0]  #number of features in X == number of rows in w\n",
    "    assert Y.shape == (1, m)  #Y -> (1, number of examples)\n",
    "    \n",
    "    #forward\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    A = np.clip(A, 1e-10, 1 - 1e-10)\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))  \n",
    "    \n",
    "    #backward\n",
    "    dw = 1/m * np.dot(X, (A - Y).T)\n",
    "    db = 1/m * np.sum(A - Y)\n",
    "    \n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d95dcdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w: np.ndarray, b: float, X: np.ndarray, Y: np.ndarray, num_iterations: int = 100, learning_rate: float = 0.009, print_cost: bool = False) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], List[float]]:\n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        #printing cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "            if print_cost:\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe08435",
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b = initialize(4)\n",
    "\n",
    "params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations=200, learning_rate=0.005, print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1a168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w: np.ndarray, b: float, X: np.ndarray) -> np.ndarray:\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b)  #activation\n",
    "    \n",
    "    for i in range(A.shape[1]):  #converting probabilities to actual predictions\n",
    "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
    "    \n",
    "    return Y_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaeb8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, num_iterations: int = 2000, learning_rate: float = 0.0001, print_cost: bool = False) -> Dict[str, Any]:\n",
    "    w, b = initialize(X_train.shape[0])\n",
    "    \n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    \n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\": Y_prediction_train, \n",
    "         \"w\": w, \n",
    "         \"b\": b,\n",
    "         \"learning_rate\": learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dad429a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.437985\n",
      "Cost after iteration 200: 0.309498\n",
      "Cost after iteration 300: 0.236510\n",
      "Cost after iteration 400: 0.190553\n",
      "Cost after iteration 500: 0.159295\n",
      "Cost after iteration 600: 0.136778\n",
      "Cost after iteration 700: 0.119835\n",
      "Cost after iteration 800: 0.106645\n",
      "Cost after iteration 900: 0.096096\n",
      "Cost after iteration 1000: 0.087472\n",
      "Cost after iteration 1100: 0.080291\n",
      "Cost after iteration 1200: 0.074221\n",
      "Cost after iteration 1300: 0.069023\n",
      "Cost after iteration 1400: 0.064521\n",
      "Cost after iteration 1500: 0.060584\n",
      "Cost after iteration 1600: 0.057113\n",
      "Cost after iteration 1700: 0.054029\n",
      "Cost after iteration 1800: 0.051270\n",
      "Cost after iteration 1900: 0.048787\n",
      "Cost after iteration 2000: 0.046542\n",
      "Cost after iteration 2100: 0.044500\n",
      "Cost after iteration 2200: 0.042636\n",
      "Cost after iteration 2300: 0.040927\n",
      "Cost after iteration 2400: 0.039355\n",
      "Cost after iteration 2500: 0.037903\n",
      "Cost after iteration 2600: 0.036558\n",
      "Cost after iteration 2700: 0.035309\n",
      "Cost after iteration 2800: 0.034146\n",
      "Cost after iteration 2900: 0.033060\n",
      "Cost after iteration 3000: 0.032044\n",
      "Cost after iteration 3100: 0.031090\n",
      "Cost after iteration 3200: 0.030194\n",
      "Cost after iteration 3300: 0.029350\n",
      "Cost after iteration 3400: 0.028554\n",
      "Cost after iteration 3500: 0.027802\n",
      "Cost after iteration 3600: 0.027091\n",
      "Cost after iteration 3700: 0.026416\n",
      "Cost after iteration 3800: 0.025775\n",
      "Cost after iteration 3900: 0.025166\n",
      "Cost after iteration 4000: 0.024587\n",
      "Cost after iteration 4100: 0.024035\n",
      "Cost after iteration 4200: 0.023508\n",
      "Cost after iteration 4300: 0.023004\n",
      "Cost after iteration 4400: 0.022523\n",
      "Cost after iteration 4500: 0.022063\n",
      "Cost after iteration 4600: 0.021621\n",
      "Cost after iteration 4700: 0.021198\n",
      "Cost after iteration 4800: 0.020792\n",
      "Cost after iteration 4900: 0.020402\n",
      "Cost after iteration 5000: 0.020027\n",
      "Cost after iteration 5100: 0.019666\n",
      "Cost after iteration 5200: 0.019319\n",
      "Cost after iteration 5300: 0.018984\n",
      "Cost after iteration 5400: 0.018661\n",
      "Cost after iteration 5500: 0.018349\n",
      "Cost after iteration 5600: 0.018049\n",
      "Cost after iteration 5700: 0.017758\n",
      "Cost after iteration 5800: 0.017477\n",
      "Cost after iteration 5900: 0.017205\n",
      "Cost after iteration 6000: 0.016942\n",
      "Cost after iteration 6100: 0.016688\n",
      "Cost after iteration 6200: 0.016441\n",
      "Cost after iteration 6300: 0.016202\n",
      "Cost after iteration 6400: 0.015970\n",
      "Cost after iteration 6500: 0.015744\n",
      "Cost after iteration 6600: 0.015526\n",
      "Cost after iteration 6700: 0.015314\n",
      "Cost after iteration 6800: 0.015107\n",
      "Cost after iteration 6900: 0.014907\n",
      "Cost after iteration 7000: 0.014712\n",
      "Cost after iteration 7100: 0.014522\n",
      "Cost after iteration 7200: 0.014338\n",
      "Cost after iteration 7300: 0.014158\n",
      "Cost after iteration 7400: 0.013983\n",
      "Cost after iteration 7500: 0.013813\n",
      "Cost after iteration 7600: 0.013647\n",
      "Cost after iteration 7700: 0.013485\n",
      "Cost after iteration 7800: 0.013327\n",
      "Cost after iteration 7900: 0.013172\n",
      "Cost after iteration 8000: 0.013022\n",
      "Cost after iteration 8100: 0.012875\n",
      "Cost after iteration 8200: 0.012732\n",
      "Cost after iteration 8300: 0.012592\n",
      "Cost after iteration 8400: 0.012455\n",
      "Cost after iteration 8500: 0.012321\n",
      "Cost after iteration 8600: 0.012190\n",
      "Cost after iteration 8700: 0.012062\n",
      "Cost after iteration 8800: 0.011937\n",
      "Cost after iteration 8900: 0.011815\n",
      "Cost after iteration 9000: 0.011695\n",
      "Cost after iteration 9100: 0.011578\n",
      "Cost after iteration 9200: 0.011463\n",
      "Cost after iteration 9300: 0.011351\n",
      "Cost after iteration 9400: 0.011241\n",
      "Cost after iteration 9500: 0.011133\n",
      "Cost after iteration 9600: 0.011027\n",
      "Cost after iteration 9700: 0.010924\n",
      "Cost after iteration 9800: 0.010822\n",
      "Cost after iteration 9900: 0.010723\n",
      "train accuracy: 100.0 %\n",
      "test accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "ogistic_model = model(X_train, Y_train, X_test, Y_test, num_iterations=10000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf978eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
