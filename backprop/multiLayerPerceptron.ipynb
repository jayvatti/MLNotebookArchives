{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1024ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "from typing import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ea25b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (569, 30)\n",
      "Shape of y: (569,)\n"
     ]
    }
   ],
   "source": [
    "breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "\n",
    "X = (X - mean) / std\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c931ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (455, 30)\n",
      "Shape of X_test: (114, 30)\n",
      "Shape of Y_train: (455,)\n",
      "Shape of Y_test: (114,)\n"
     ]
    }
   ],
   "source": [
    "#splitting into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=55)\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "print(\"Shape of Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7923b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.T \n",
    "X_test = X_test.T\n",
    "\n",
    "Y_train = Y_train.reshape(1, -1)\n",
    "Y_test = Y_test.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "130a9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (30, 455)\n",
      "Shape of X_test: (30, 114)\n",
      "Shape of Y_train: (1, 455)\n",
      "Shape of Y_test: (1, 114)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "print(\"Shape of Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e84177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims: List[int]) -> Dict[str, np.ndarray]:\n",
    "    \n",
    "    np.random.seed(5)\n",
    "    parameters: Dict[str, np.ndarray] = {}\n",
    "    L = len(layer_dims) #number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        #initializing parameters\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0bbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def linear_forward(A: np.ndarray, W: np.ndarray, b: np.ndarray) -> Tuple[np.ndarray, Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    Z = np.dot(W, A) + b #A is the activations from the previous layer\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e93a829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A, Z\n",
    "\n",
    "def relu(Z: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    A = np.maximum(0, Z)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e0a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev: np.ndarray, W: np.ndarray, b: np.ndarray, activation: str) -> Tuple[np.ndarray, Tuple[Tuple[np.ndarray, np.ndarray, np.ndarray], np.ndarray]]:\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ca4f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X: np.ndarray, parameters: Dict[str, np.ndarray]) -> Tuple[np.ndarray, List[Tuple[Tuple[np.ndarray, np.ndarray, np.ndarray], np.ndarray]]]:\n",
    "    #AL: activation value from the last layer\n",
    "    #caches: every cache of linear_activation_forward() (L caches indexed from 0 to L-1)\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  #number of layers in the neural network\n",
    "\n",
    "    #[LINEAR -> RELU]*(L-1). adding \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    #LINEAR -> SIGMOID. adding \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef103f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL: np.ndarray, Y: np.ndarray) -> Union[float, np.ndarray]:\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = -1/m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "    \n",
    "    cost = np.squeeze(cost)  \n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809747a",
   "metadata": {},
   "source": [
    "\\[\n",
    "\\begin{align*}\n",
    "dW^{[l]} &= \\frac{\\partial \\mathcal{J}}{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1]^T} \\\\\n",
    "db^{[l]} &= \\frac{\\partial \\mathcal{J}}{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[l](i)} \\\\\n",
    "dA^{[l-1]} &= \\frac{\\partial \\mathcal{L}}{\\partial A^{[l-1]}} = W^{[l]^T} dZ^{[l]}\n",
    "\\end{align*}\n",
    "\\]\n",
    "\\begin{align}\n",
    "dZ^{[l]} = dA^{[l]} \\odot g'(Z^{[l]}).\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "110172d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ: np.ndarray, cache: Tuple[np.ndarray, np.ndarray, np.ndarray]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    #computing the gradients\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "525d4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA: np.ndarray, activation_cache: np.ndarray) -> np.ndarray:\n",
    "    Z = activation_cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA: np.ndarray, activation_cache: np.ndarray) -> np.ndarray:\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "    #when z <= 0 -> dz is 0 \n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2327ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA: np.ndarray, cache: Tuple[Tuple[np.ndarray, np.ndarray, np.ndarray], np.ndarray], activation: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function: {}\".format(activation))\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f57e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL: np.ndarray, Y: np.ndarray, caches: List[Tuple[Tuple[np.ndarray, np.ndarray, np.ndarray], np.ndarray]]) -> Dict[str, np.ndarray]:\n",
    "    \n",
    "    #AL: probability vector, output of the forward propagation (L_model_forward())\n",
    "    #caches: linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "        # : the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    grads = {}\n",
    "    L = len(caches) #the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    #initializing backprop\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "    \n",
    "    #Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)]\n",
    "    current_cache = caches[-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    #looping from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        #lth layer: (RELU -> LINEAR) gradients.\n",
    "        #Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)], grads[\"dW\" + str(l + 1)], grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "780660ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params: Dict[str, np.ndarray], grads: Dict[str, np.ndarray], learning_rate: float) -> Dict[str, np.ndarray]:\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2  \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ac8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X: np.ndarray, Y: np.ndarray, layers_dims: List[int], learning_rate: float = 0.01, num_iterations: int = 10000, print_cost: bool = False) -> Dict[str, np.ndarray]:\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba7dc0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [30, 120, 120, 512, 60, 30, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79d3656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147152941962\n",
      "Cost after iteration 1000: 0.6527453316270071\n",
      "Cost after iteration 2000: 0.6524031307248154\n",
      "Cost after iteration 3000: 0.6523997764838727\n",
      "Cost after iteration 4000: 0.6523997255392932\n",
      "Cost after iteration 5000: 0.6523997102755271\n",
      "Cost after iteration 6000: 0.65239969759289\n",
      "Cost after iteration 7000: 0.652399686555513\n",
      "Cost after iteration 8000: 0.652399676842074\n",
      "Cost after iteration 9000: 0.6523996682035341\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, layers_dims, num_iterations=10000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57d9751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, parameters: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    AL, _ = L_model_forward(X, parameters)\n",
    "    predictions = (AL > 0.5).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e84e637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X: np.ndarray, Y: np.ndarray, parameters: Dict[str, np.ndarray]) -> float:\n",
    "    predictions = predict(X, parameters)\n",
    "    accuracy = np.mean(predictions == Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ec66420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1]]\n",
      "Ground Truth: [[0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1\n",
      "  1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1\n",
      "  1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
      "  1 0 0 1 1 1]]\n",
      "Accuracy: 0.5701754385964912\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X_test, parameters)\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"Ground Truth: {Y_test}\")\n",
    "    \n",
    "accuracy = evaluate(X_test, Y_test, parameters)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e12ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
